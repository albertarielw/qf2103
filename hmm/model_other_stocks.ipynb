{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion: \n",
    "\n",
    "Feature Selection: CCI, ADX, Chaikin A/D\n",
    "Model Selection: best using SVC with poly kernel\n",
    "Hyperparameter Tuning: d = 2 c = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CONST\n",
    "'''\n",
    "FILEPATH_STOCK_PRICE = \"./tr_eikon_eod_data.csv\"\n",
    "STOCK_NAMES = ['AAPL.O', 'MSFT.O', 'INTC.O', 'AMZN.O', 'GS.N']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "UTILS\n",
    "'''\n",
    "def sequential_train_test_split(data, ratio = 0.8):\n",
    "    df = pd.DataFrame(data.dropna()).to_numpy()\n",
    "    split_index = int(len(df) * ratio)\n",
    "    return df[:split_index], df[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BENCHMARK\n",
    "'''\n",
    "\n",
    "def benchmark_accuracy(prediction, test):\n",
    "    prediction = pd.DataFrame(prediction)\n",
    "    test = pd.DataFrame(test)\n",
    "\n",
    "    return (prediction == test).sum() / len(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "MODEL\n",
    "'''\n",
    "\n",
    "def predict_position(train, test):\n",
    "    # Train Hidden Markov Model\n",
    "    model = hmm.GaussianHMM(n_components=2, covariance_type=\"diag\", n_iter=1000)\n",
    "    model.fit(train.reshape(-1, 1))\n",
    "\n",
    "    # Predict hidden states for test data\n",
    "    hidden_states = model.predict(test.reshape(-1, 1))\n",
    "\n",
    "    # Define a function to map hidden states to rise or fall\n",
    "    def map_to_rise_or_fall(state):\n",
    "        return 1 if state == 0 else -1\n",
    "\n",
    "    # Map hidden states to rise or fall\n",
    "    predicted_rise_fall = np.array(list(map(map_to_rise_or_fall, hidden_states)))\n",
    "\n",
    "    return predicted_rise_fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GS.N</th>\n",
       "      <th>SlowK</th>\n",
       "      <th>SlowD</th>\n",
       "      <th>RSI</th>\n",
       "      <th>ADX</th>\n",
       "      <th>CCI</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>OBV</th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "      <th>SMA_EMA_ratio</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-01</th>\n",
       "      <td>173.08</td>\n",
       "      <td>9.7641</td>\n",
       "      <td>15.2302</td>\n",
       "      <td>50.3525</td>\n",
       "      <td>25.5682</td>\n",
       "      <td>-119.6258</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>64.2857</td>\n",
       "      <td>-177810600.0</td>\n",
       "      <td>8.118063e+08</td>\n",
       "      <td>137.0128</td>\n",
       "      <td>135.9827</td>\n",
       "      <td>1.007575</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-01</th>\n",
       "      <td>176.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-01</th>\n",
       "      <td>174.26</td>\n",
       "      <td>82.1737</td>\n",
       "      <td>80.0595</td>\n",
       "      <td>49.2538</td>\n",
       "      <td>31.4835</td>\n",
       "      <td>85.6424</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>14.2857</td>\n",
       "      <td>-346418300.0</td>\n",
       "      <td>6.644775e+08</td>\n",
       "      <td>110.7708</td>\n",
       "      <td>112.1395</td>\n",
       "      <td>0.987795</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-01</th>\n",
       "      <td>177.67</td>\n",
       "      <td>9.8352</td>\n",
       "      <td>26.4734</td>\n",
       "      <td>37.8543</td>\n",
       "      <td>33.0676</td>\n",
       "      <td>-128.0550</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>-430663700.0</td>\n",
       "      <td>6.215069e+08</td>\n",
       "      <td>107.1164</td>\n",
       "      <td>106.6629</td>\n",
       "      <td>1.004252</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25</th>\n",
       "      <td>221.54</td>\n",
       "      <td>19.9620</td>\n",
       "      <td>20.9447</td>\n",
       "      <td>32.7234</td>\n",
       "      <td>22.7421</td>\n",
       "      <td>-152.1742</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>42.8571</td>\n",
       "      <td>-240254198.0</td>\n",
       "      <td>6.437021e+08</td>\n",
       "      <td>201.4811</td>\n",
       "      <td>200.4351</td>\n",
       "      <td>1.005219</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26</th>\n",
       "      <td>221.58</td>\n",
       "      <td>17.8048</td>\n",
       "      <td>19.8747</td>\n",
       "      <td>28.5329</td>\n",
       "      <td>24.4674</td>\n",
       "      <td>-140.4348</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>35.7143</td>\n",
       "      <td>-237561311.0</td>\n",
       "      <td>6.446095e+08</td>\n",
       "      <td>200.4011</td>\n",
       "      <td>199.2830</td>\n",
       "      <td>1.005611</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27</th>\n",
       "      <td>220.18</td>\n",
       "      <td>14.1021</td>\n",
       "      <td>17.2896</td>\n",
       "      <td>28.0921</td>\n",
       "      <td>24.3184</td>\n",
       "      <td>-105.2860</td>\n",
       "      <td>92.8571</td>\n",
       "      <td>28.5714</td>\n",
       "      <td>-240664932.0</td>\n",
       "      <td>6.415525e+08</td>\n",
       "      <td>199.3360</td>\n",
       "      <td>198.2927</td>\n",
       "      <td>1.005261</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-28</th>\n",
       "      <td>223.42</td>\n",
       "      <td>22.9057</td>\n",
       "      <td>18.2709</td>\n",
       "      <td>26.2447</td>\n",
       "      <td>24.3965</td>\n",
       "      <td>-71.9738</td>\n",
       "      <td>85.7143</td>\n",
       "      <td>21.4286</td>\n",
       "      <td>-237592302.0</td>\n",
       "      <td>6.431251e+08</td>\n",
       "      <td>197.9690</td>\n",
       "      <td>197.2867</td>\n",
       "      <td>1.003458</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-29</th>\n",
       "      <td>220.57</td>\n",
       "      <td>23.3116</td>\n",
       "      <td>20.1065</td>\n",
       "      <td>40.8312</td>\n",
       "      <td>24.1276</td>\n",
       "      <td>-65.5979</td>\n",
       "      <td>78.5714</td>\n",
       "      <td>14.2857</td>\n",
       "      <td>-240987565.0</td>\n",
       "      <td>6.399025e+08</td>\n",
       "      <td>197.3476</td>\n",
       "      <td>197.1240</td>\n",
       "      <td>1.001134</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2216 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              GS.N    SlowK    SlowD      RSI      ADX       CCI  Aroon Down  \\\n",
       "Date                                                                           \n",
       "2010-01-01     NaN      NaN      NaN      NaN      NaN       NaN         NaN   \n",
       "2010-04-01  173.08   9.7641  15.2302  50.3525  25.5682 -119.6258    100.0000   \n",
       "2010-05-01  176.14      NaN      NaN      NaN      NaN       NaN         NaN   \n",
       "2010-06-01  174.26  82.1737  80.0595  49.2538  31.4835   85.6424     71.4286   \n",
       "2010-07-01  177.67   9.8352  26.4734  37.8543  33.0676 -128.0550    100.0000   \n",
       "...            ...      ...      ...      ...      ...       ...         ...   \n",
       "2018-06-25  221.54  19.9620  20.9447  32.7234  22.7421 -152.1742    100.0000   \n",
       "2018-06-26  221.58  17.8048  19.8747  28.5329  24.4674 -140.4348    100.0000   \n",
       "2018-06-27  220.18  14.1021  17.2896  28.0921  24.3184 -105.2860     92.8571   \n",
       "2018-06-28  223.42  22.9057  18.2709  26.2447  24.3965  -71.9738     85.7143   \n",
       "2018-06-29  220.57  23.3116  20.1065  40.8312  24.1276  -65.5979     78.5714   \n",
       "\n",
       "            Aroon Up          OBV   Chaikin A/D       SMA       EMA  \\\n",
       "Date                                                                  \n",
       "2010-01-01       NaN          NaN           NaN       NaN       NaN   \n",
       "2010-04-01   64.2857 -177810600.0  8.118063e+08  137.0128  135.9827   \n",
       "2010-05-01       NaN          NaN           NaN       NaN       NaN   \n",
       "2010-06-01   14.2857 -346418300.0  6.644775e+08  110.7708  112.1395   \n",
       "2010-07-01   71.4286 -430663700.0  6.215069e+08  107.1164  106.6629   \n",
       "...              ...          ...           ...       ...       ...   \n",
       "2018-06-25   42.8571 -240254198.0  6.437021e+08  201.4811  200.4351   \n",
       "2018-06-26   35.7143 -237561311.0  6.446095e+08  200.4011  199.2830   \n",
       "2018-06-27   28.5714 -240664932.0  6.415525e+08  199.3360  198.2927   \n",
       "2018-06-28   21.4286 -237592302.0  6.431251e+08  197.9690  197.2867   \n",
       "2018-06-29   14.2857 -240987565.0  6.399025e+08  197.3476  197.1240   \n",
       "\n",
       "            SMA_EMA_ratio  direction  \n",
       "Date                                  \n",
       "2010-01-01            NaN         -1  \n",
       "2010-04-01       1.007575          1  \n",
       "2010-05-01            NaN         -1  \n",
       "2010-06-01       0.987795          1  \n",
       "2010-07-01       1.004252         -1  \n",
       "...                   ...        ...  \n",
       "2018-06-25       1.005219          1  \n",
       "2018-06-26       1.005611         -1  \n",
       "2018-06-27       1.005261          1  \n",
       "2018-06-28       1.003458         -1  \n",
       "2018-06-29       1.001134         -1  \n",
       "\n",
       "[2216 rows x 14 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'gs_output.csv'\n",
    "stockname = 'GS.N'\n",
    "\n",
    "apple_data = pd.read_csv(filename, index_col = 0, parse_dates = True)\n",
    "apple_data['SMA_EMA_ratio'] = apple_data['SMA'] / apple_data['EMA']\n",
    "apple_data['direction'] = np.where(apple_data[stockname] <= apple_data[stockname].shift(-1), 1, -1)\n",
    "\n",
    "apple_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMZN.O</th>\n",
       "      <th>SlowK</th>\n",
       "      <th>SlowD</th>\n",
       "      <th>RSI</th>\n",
       "      <th>ADX</th>\n",
       "      <th>CCI</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>OBV</th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "      <th>SMA_EMA_ratio</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-01</th>\n",
       "      <td>0.015407</td>\n",
       "      <td>0.547956</td>\n",
       "      <td>0.743189</td>\n",
       "      <td>0.642768</td>\n",
       "      <td>0.470929</td>\n",
       "      <td>0.506203</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.125422</td>\n",
       "      <td>0.082504</td>\n",
       "      <td>0.010625</td>\n",
       "      <td>0.010581</td>\n",
       "      <td>0.445655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-01</th>\n",
       "      <td>0.015888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-01</th>\n",
       "      <td>0.014402</td>\n",
       "      <td>0.745369</td>\n",
       "      <td>0.803009</td>\n",
       "      <td>0.359953</td>\n",
       "      <td>0.656789</td>\n",
       "      <td>0.511902</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.094772</td>\n",
       "      <td>0.081320</td>\n",
       "      <td>0.005648</td>\n",
       "      <td>0.005986</td>\n",
       "      <td>0.349605</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-01</th>\n",
       "      <td>0.013031</td>\n",
       "      <td>0.186826</td>\n",
       "      <td>0.158464</td>\n",
       "      <td>0.047981</td>\n",
       "      <td>0.663867</td>\n",
       "      <td>0.317693</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.048608</td>\n",
       "      <td>0.064585</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.810013</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25</th>\n",
       "      <td>0.947041</td>\n",
       "      <td>0.253876</td>\n",
       "      <td>0.500732</td>\n",
       "      <td>0.476692</td>\n",
       "      <td>0.662018</td>\n",
       "      <td>0.191420</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.898056</td>\n",
       "      <td>0.983911</td>\n",
       "      <td>0.863689</td>\n",
       "      <td>0.859967</td>\n",
       "      <td>0.543386</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26</th>\n",
       "      <td>0.964063</td>\n",
       "      <td>0.221629</td>\n",
       "      <td>0.322018</td>\n",
       "      <td>0.353668</td>\n",
       "      <td>0.612456</td>\n",
       "      <td>0.308819</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.904173</td>\n",
       "      <td>0.986523</td>\n",
       "      <td>0.862580</td>\n",
       "      <td>0.855759</td>\n",
       "      <td>0.599412</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27</th>\n",
       "      <td>0.945433</td>\n",
       "      <td>0.178958</td>\n",
       "      <td>0.216319</td>\n",
       "      <td>0.490156</td>\n",
       "      <td>0.553130</td>\n",
       "      <td>0.320307</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.897378</td>\n",
       "      <td>0.980226</td>\n",
       "      <td>0.862865</td>\n",
       "      <td>0.855806</td>\n",
       "      <td>0.603685</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-28</th>\n",
       "      <td>0.970374</td>\n",
       "      <td>0.327892</td>\n",
       "      <td>0.242204</td>\n",
       "      <td>0.366393</td>\n",
       "      <td>0.499738</td>\n",
       "      <td>0.403548</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.903695</td>\n",
       "      <td>0.985106</td>\n",
       "      <td>0.860662</td>\n",
       "      <td>0.852373</td>\n",
       "      <td>0.626370</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-29</th>\n",
       "      <td>0.969369</td>\n",
       "      <td>0.436374</td>\n",
       "      <td>0.317304</td>\n",
       "      <td>0.504979</td>\n",
       "      <td>0.439608</td>\n",
       "      <td>0.481419</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.897359</td>\n",
       "      <td>0.981373</td>\n",
       "      <td>0.860824</td>\n",
       "      <td>0.853903</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2216 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              AMZN.O     SlowK     SlowD       RSI       ADX       CCI  \\\n",
       "Date                                                                     \n",
       "2010-01-01       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2010-04-01  0.015407  0.547956  0.743189  0.642768  0.470929  0.506203   \n",
       "2010-05-01  0.015888       NaN       NaN       NaN       NaN       NaN   \n",
       "2010-06-01  0.014402  0.745369  0.803009  0.359953  0.656789  0.511902   \n",
       "2010-07-01  0.013031  0.186826  0.158464  0.047981  0.663867  0.317693   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2018-06-25  0.947041  0.253876  0.500732  0.476692  0.662018  0.191420   \n",
       "2018-06-26  0.964063  0.221629  0.322018  0.353668  0.612456  0.308819   \n",
       "2018-06-27  0.945433  0.178958  0.216319  0.490156  0.553130  0.320307   \n",
       "2018-06-28  0.970374  0.327892  0.242204  0.366393  0.499738  0.403548   \n",
       "2018-06-29  0.969369  0.436374  0.317304  0.504979  0.439608  0.481419   \n",
       "\n",
       "            Aroon Down  Aroon Up       OBV  Chaikin A/D       SMA       EMA  \\\n",
       "Date                                                                          \n",
       "2010-01-01         NaN       NaN       NaN          NaN       NaN       NaN   \n",
       "2010-04-01    0.571429  0.857143  0.125422     0.082504  0.010625  0.010581   \n",
       "2010-05-01         NaN       NaN       NaN          NaN       NaN       NaN   \n",
       "2010-06-01    0.571429  0.142857  0.094772     0.081320  0.005648  0.005986   \n",
       "2010-07-01    0.857143  0.214286  0.048608     0.064585  0.003012  0.001564   \n",
       "...                ...       ...       ...          ...       ...       ...   \n",
       "2018-06-25    1.000000  0.857143  0.898056     0.983911  0.863689  0.859967   \n",
       "2018-06-26    0.928571  0.785714  0.904173     0.986523  0.862580  0.855759   \n",
       "2018-06-27    0.857143  0.714286  0.897378     0.980226  0.862865  0.855806   \n",
       "2018-06-28    0.785714  0.642857  0.903695     0.985106  0.860662  0.852373   \n",
       "2018-06-29    0.714286  0.571429  0.897359     0.981373  0.860824  0.853903   \n",
       "\n",
       "            SMA_EMA_ratio  direction  \n",
       "Date                                  \n",
       "2010-01-01            NaN         -1  \n",
       "2010-04-01       0.445655          1  \n",
       "2010-05-01            NaN         -1  \n",
       "2010-06-01       0.349605         -1  \n",
       "2010-07-01       0.810013          1  \n",
       "...                   ...        ...  \n",
       "2018-06-25       0.543386          1  \n",
       "2018-06-26       0.599412         -1  \n",
       "2018-06-27       0.603685          1  \n",
       "2018-06-28       0.626370         -1  \n",
       "2018-06-29       0.601449         -1  \n",
       "\n",
       "[2216 rows x 14 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the selected columns\n",
    "normalized_columns = scaler.fit_transform(apple_data.iloc[:, :-1])\n",
    "\n",
    "# Create a new DataFrame with the normalized values\n",
    "normalized_df = pd.DataFrame(normalized_columns, columns=apple_data.columns[:-1])\n",
    "\n",
    "# Display the normalized DataFrame\n",
    "apple_data.iloc[:, :-1] = normalized_df\n",
    "apple_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apple_data = apple_data[['SlowK', 'RSI', 'CCI', 'Aroon Down', 'OBV', 'direction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = sequential_train_test_split(apple_data)\n",
    "X_train, y_train, X_test, y_test = train_data[:, :-1], train_data[:, -1], test_data[:, :-1], test_data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01883667, 0.79708231, 0.85194069, ..., 0.02191803, 0.02147607,\n",
       "        0.43614585],\n",
       "       [0.01622904, 0.9071284 , 0.84424684, ..., 0.0337416 , 0.03448778,\n",
       "        0.26904463],\n",
       "       [0.01589451, 0.11149665, 0.1094004 , ..., 0.04589134, 0.04162706,\n",
       "        0.95070081],\n",
       "       ...,\n",
       "       [0.53766059, 0.68329218, 0.72737065, ..., 0.4305807 , 0.43008523,\n",
       "        0.43860904],\n",
       "       [0.54156345, 0.72807436, 0.72102435, ..., 0.4332005 , 0.43240882,\n",
       "        0.45046602],\n",
       "       [0.5410831 , 0.67998392, 0.70044166, ..., 0.43543349, 0.43382752,\n",
       "        0.48296742]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.494709\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(kernel='rbf', gamma=0.7, C=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "pred = model.predict(X_test)\n",
    "benchmark_accuracy(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5291005291005291, d: 1, c: 1\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5291005291005291, d: 1, c: 1\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5291005291005291, d: 1, c: 1\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5291005291005291, d: 1, c: 1\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5291005291005291, d: 1, c: 1\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5291005291005291, d: 1, c: 1\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5291005291005291, d: 1, c: 1\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5291005291005291, d: 1, c: 1\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5291005291005291, d: 1, c: 1\n",
      "acc: 0.534 up: 0.989\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.529 up: 0.979\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.532 up: 0.96\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.521 up: 0.939\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.516 up: 0.902\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.508 up: 0.894\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.508 up: 0.894\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.503 up: 0.873\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.497 up: 0.862\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.497 up: 0.868\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.497 up: 0.868\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.497 up: 0.868\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.495 up: 0.86\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.495 up: 0.86\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.497 up: 0.857\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.503 up: 0.847\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.5 up: 0.844\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.503 up: 0.841\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.505 up: 0.839\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.823\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.81\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.495 up: 0.807\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.802\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.794\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.495 up: 0.791\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.495 up: 0.791\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.497 up: 0.788\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.778\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.772\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.484 up: 0.759\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.471 up: 0.746\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.468 up: 0.743\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.471 up: 0.741\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.471 up: 0.741\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.471 up: 0.741\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.479 up: 0.754\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.481 up: 0.751\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.479 up: 0.749\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.479 up: 0.749\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.479 up: 0.749\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.532 up: 0.992\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.532 up: 0.966\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.519 up: 0.915\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.505 up: 0.886\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.497 up: 0.868\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.497 up: 0.868\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.497 up: 0.862\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.495 up: 0.854\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.82\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.495 up: 0.807\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.804\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.802\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.794\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.778\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.495 up: 0.775\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.484 up: 0.759\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.481 up: 0.762\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.484 up: 0.765\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.762\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.765\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.762\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.762\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.762\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.765\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.765\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.765\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.772\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.772\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.775\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.775\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.772\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.765\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.529 up: 1.0\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.532 up: 0.966\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.505 up: 0.886\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.503 up: 0.873\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.497 up: 0.868\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.495 up: 0.828\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.817\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.794\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.78\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.765\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.762\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.765\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.487 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.489 up: 0.77\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.772\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.772\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.495 up: 0.775\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n",
      "acc: 0.492 up: 0.767\n",
      "best acc: 0.5343915343915344, d: 1, c: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m curr_X_test \u001b[38;5;241m=\u001b[39m X_test[:, selected_features]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(selected_features))\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, gamma\u001b[38;5;241m=\u001b[39mg \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m, C\u001b[38;5;241m=\u001b[39mc \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(curr_X_test)\n\u001b[1;32m     14\u001b[0m up_percentage \u001b[38;5;241m=\u001b[39m ((pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(pred))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[1;32m    319\u001b[0m (\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[0;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_g = -1\n",
    "best_c = -1\n",
    "\n",
    "selected_features = [4, 5, 9]\n",
    "\n",
    "for g in range(1, 11, 1):\n",
    "    for c in range(1, 51, 1):\n",
    "        curr_X_train = X_train[:, selected_features].reshape(-1, len(selected_features))\n",
    "        curr_X_test = X_test[:, selected_features].reshape(-1, len(selected_features))\n",
    "        model = SVC(kernel='rbf', gamma=g / 10, C=c / 10)\n",
    "        model.fit(curr_X_train, y_train)\n",
    "        pred = model.predict(curr_X_test)\n",
    "        up_percentage = ((pred > 0).sum() / len(pred))\n",
    "        acc = benchmark_accuracy(pred, y_test)\n",
    "        print(f\"acc: {round(acc[0], 3)} up: {round(up_percentage, 3)}\")\n",
    "\n",
    "        if acc[0] > best_acc:\n",
    "            best_acc = acc[0]\n",
    "            best_g = g\n",
    "            best_c = c\n",
    "        print(f\"best acc: {best_acc}, d: {best_g}, c: {best_c}\")\n",
    "    \n",
    "print(f\"best acc: {best_acc}, g: {best_g}, c: {best_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.519 up: 0.0\n",
      "best acc: 0.5185185185185185, c: 1\n",
      "acc: 0.519 up: 0.0\n",
      "best acc: 0.5185185185185185, c: 1\n",
      "acc: 0.521 up: 0.198\n",
      "best acc: 0.5211640211640212, c: 3\n",
      "acc: 0.532 up: 0.098\n",
      "best acc: 0.5317460317460317, c: 4\n",
      "acc: 0.542 up: 0.743\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.537 up: 0.749\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.503 up: 0.037\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.489 up: 0.071\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.495 up: 0.082\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.487 up: 0.101\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.484 up: 0.108\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.497 up: 0.132\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.481 up: 0.148\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.481 up: 0.159\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.172\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.18\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.463 up: 0.188\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.463 up: 0.204\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.466 up: 0.222\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.225\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.471 up: 0.228\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.471 up: 0.238\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.466 up: 0.249\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.254\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.458 up: 0.257\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.452 up: 0.262\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.447 up: 0.267\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.447 up: 0.267\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.447 up: 0.267\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.447 up: 0.278\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.452 up: 0.283\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.458 up: 0.288\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.458 up: 0.288\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.458 up: 0.288\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.458 up: 0.288\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.458 up: 0.288\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.291\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.291\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.291\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.291\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.291\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.291\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.291\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.291\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.291\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.296\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.296\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.458 up: 0.299\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.458 up: 0.294\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.458 up: 0.294\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.455 up: 0.296\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.302\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.302\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.458 up: 0.299\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.302\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.302\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.302\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.302\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.302\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.302\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.46 up: 0.302\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.463 up: 0.304\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.463 up: 0.304\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.463 up: 0.304\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.463 up: 0.304\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "acc: 0.468 up: 0.31\n",
      "best acc: 0.5423280423280423, c: 5\n",
      "best acc: 0.5423280423280423, c: 5\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_c = -1\n",
    "\n",
    "selected_features = [4, 5, 9]\n",
    "\n",
    "for c in range(1, 101, 1):\n",
    "    curr_X_train = X_train[:, selected_features].reshape(-1, len(selected_features))\n",
    "    curr_X_test = X_test[:, selected_features].reshape(-1, len(selected_features))\n",
    "    model = SVC(kernel='sigmoid', C= c/10)\n",
    "    model.fit(curr_X_train, y_train)\n",
    "    pred = model.predict(curr_X_test)\n",
    "    up_percentage = ((pred > 0).sum() / len(pred))\n",
    "    acc = benchmark_accuracy(pred, y_test)\n",
    "    print(f\"acc: {round(acc[0], 3)} up: {round(up_percentage, 3)}\")\n",
    "\n",
    "    if acc[0] > best_acc:\n",
    "        best_acc = acc[0]\n",
    "        best_g = g\n",
    "        best_c = c\n",
    "    print(f\"best acc: {best_acc}, c: {best_c}\")\n",
    "    \n",
    "print(f\"best acc: {best_acc}, c: {best_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.468 up: 0.743\n",
      "best acc: 0.46825396825396826, d: 1, c: 1\n",
      "acc: 0.476 up: 0.73\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.468 up: 0.712\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.714\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.714\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.714\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.714\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.714\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.698\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.474 up: 0.696\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.698\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.474 up: 0.696\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.698\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.698\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.474 up: 0.696\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.471 up: 0.693\n",
      "best acc: 0.47619047619047616, d: 1, c: 2\n",
      "acc: 0.484 up: 0.775\n",
      "best acc: 0.48412698412698413, d: 2, c: 1\n",
      "acc: 0.484 up: 0.775\n",
      "best acc: 0.48412698412698413, d: 2, c: 1\n",
      "acc: 0.492 up: 0.794\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.794\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.794\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.794\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.796\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.802\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.802\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.802\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.802\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.802\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.489 up: 0.802\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.492 up: 0.799\n",
      "best acc: 0.49206349206349204, d: 2, c: 3\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.516 up: 0.934\n",
      "best acc: 0.5158730158730159, d: 3, c: 1\n",
      "acc: 0.521 up: 0.876\n",
      "best acc: 0.5211640211640212, d: 4, c: 1\n",
      "acc: 0.521 up: 0.854\n",
      "best acc: 0.5211640211640212, d: 4, c: 1\n",
      "acc: 0.524 up: 0.847\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.847\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.847\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.847\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.849\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.849\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.849\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.849\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.849\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.849\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.849\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.849\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.847\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.847\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.844\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.844\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.524 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.839\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.839\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.839\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.839\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.839\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.839\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.839\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.839\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.521 up: 0.839\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.519 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.519 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.519 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.519 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.519 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "acc: 0.519 up: 0.841\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n",
      "best acc: 0.5238095238095238, d: 4, c: 3\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_d = -1\n",
    "best_c = -1\n",
    "\n",
    "selected_features = [4, 5, 9]\n",
    "\n",
    "for d in range(1, 5, 1):\n",
    "    for c in range(1, 51, 1):\n",
    "        curr_X_train = X_train[:, selected_features].reshape(-1, len(selected_features))\n",
    "        curr_X_test = X_test[:, selected_features].reshape(-1, len(selected_features))\n",
    "        model = SVC(kernel='poly', degree=d, C=c/10)\n",
    "        model.fit(curr_X_train, y_train)\n",
    "        pred = model.predict(curr_X_test)\n",
    "        up_percentage = ((pred > 0).sum() / len(pred))\n",
    "        acc = benchmark_accuracy(pred, y_test)\n",
    "        print(f\"acc: {round(acc[0], 3)} up: {round(up_percentage, 3)}\")\n",
    "\n",
    "        if acc[0] > best_acc:\n",
    "            best_acc = acc[0]\n",
    "            best_d = d\n",
    "            best_c = c\n",
    "        print(f\"best acc: {best_acc}, d: {best_d}, c: {best_c}\")\n",
    "    \n",
    "print(f\"best acc: {best_acc}, d: {best_d}, c: {best_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.619 up: 0.28\n",
      "best acc: 0.6190476190476191, c: 1\n",
      "acc: 0.611 up: 0.257\n",
      "best acc: 0.6190476190476191, c: 1\n",
      "acc: 0.622 up: 0.246\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.606 up: 0.235\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.608 up: 0.238\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.606 up: 0.235\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.606 up: 0.235\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.608 up: 0.233\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.608 up: 0.233\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.608 up: 0.228\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.608 up: 0.228\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.611 up: 0.23\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.598 up: 0.217\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.603 up: 0.222\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "acc: 0.601 up: 0.22\n",
      "best acc: 0.6216931216931217, c: 3\n",
      "best acc: 0.6216931216931217, c: 3\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "best_c = -1\n",
    "\n",
    "selected_features = [4, 5, 9]\n",
    "\n",
    "# for d in range(1, 5, 1):\n",
    "for c in range(1, 101, 1):\n",
    "    curr_X_train = X_train[:, selected_features].reshape(-1, len(selected_features))\n",
    "    curr_X_test = X_test[:, selected_features].reshape(-1, len(selected_features))\n",
    "    model = SVC(kernel='poly', degree=2, C = c / 10)\n",
    "    model.fit(curr_X_train, y_train)\n",
    "    pred = model.predict(curr_X_test)\n",
    "    up_percentage = ((pred > 0).sum() / len(pred))\n",
    "    acc = benchmark_accuracy(pred, y_test)\n",
    "    print(f\"acc: {round(acc[0], 3)} up: {round(up_percentage, 3)}\")\n",
    "\n",
    "    if acc[0] > best_acc:\n",
    "        best_acc = acc[0]\n",
    "        best_c = c\n",
    "    print(f\"best acc: {best_acc}, c: {best_c}\")\n",
    "    \n",
    "print(f\"best acc: {best_acc}, c: {best_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = apple_data.dropna().iloc[:, :-1]\n",
    "y = apple_data.dropna().iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL.O</th>\n",
       "      <th>SlowK</th>\n",
       "      <th>SlowD</th>\n",
       "      <th>RSI</th>\n",
       "      <th>ADX</th>\n",
       "      <th>CCI</th>\n",
       "      <th>Aroon Down</th>\n",
       "      <th>Aroon Up</th>\n",
       "      <th>OBV</th>\n",
       "      <th>Chaikin A/D</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "      <th>SMA_EMA_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-04-01</th>\n",
       "      <td>0.018837</td>\n",
       "      <td>0.797082</td>\n",
       "      <td>0.851941</td>\n",
       "      <td>0.769280</td>\n",
       "      <td>0.413634</td>\n",
       "      <td>0.658869</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.466485</td>\n",
       "      <td>0.197673</td>\n",
       "      <td>0.021918</td>\n",
       "      <td>0.021476</td>\n",
       "      <td>0.436146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-01</th>\n",
       "      <td>0.016229</td>\n",
       "      <td>0.907128</td>\n",
       "      <td>0.844247</td>\n",
       "      <td>0.536156</td>\n",
       "      <td>0.374250</td>\n",
       "      <td>0.748379</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.417465</td>\n",
       "      <td>0.386577</td>\n",
       "      <td>0.033742</td>\n",
       "      <td>0.034488</td>\n",
       "      <td>0.269045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-01</th>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.111497</td>\n",
       "      <td>0.109400</td>\n",
       "      <td>0.287327</td>\n",
       "      <td>0.221484</td>\n",
       "      <td>0.249224</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.311572</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.045891</td>\n",
       "      <td>0.041627</td>\n",
       "      <td>0.950701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-11-01</th>\n",
       "      <td>0.015491</td>\n",
       "      <td>0.258310</td>\n",
       "      <td>0.233819</td>\n",
       "      <td>0.445504</td>\n",
       "      <td>0.149730</td>\n",
       "      <td>0.329648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.566643</td>\n",
       "      <td>0.506703</td>\n",
       "      <td>0.070674</td>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.680743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-12-01</th>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.723159</td>\n",
       "      <td>0.787620</td>\n",
       "      <td>0.558717</td>\n",
       "      <td>0.137354</td>\n",
       "      <td>0.660432</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.561133</td>\n",
       "      <td>0.530421</td>\n",
       "      <td>0.072551</td>\n",
       "      <td>0.072439</td>\n",
       "      <td>0.398836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-25</th>\n",
       "      <td>0.929088</td>\n",
       "      <td>0.244971</td>\n",
       "      <td>0.261688</td>\n",
       "      <td>0.215016</td>\n",
       "      <td>0.272481</td>\n",
       "      <td>0.283454</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.572554</td>\n",
       "      <td>0.836229</td>\n",
       "      <td>0.812646</td>\n",
       "      <td>0.803231</td>\n",
       "      <td>0.652194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-26</th>\n",
       "      <td>0.942658</td>\n",
       "      <td>0.308398</td>\n",
       "      <td>0.263699</td>\n",
       "      <td>0.201690</td>\n",
       "      <td>0.271435</td>\n",
       "      <td>0.383573</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.576126</td>\n",
       "      <td>0.836040</td>\n",
       "      <td>0.808467</td>\n",
       "      <td>0.799351</td>\n",
       "      <td>0.645859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-27</th>\n",
       "      <td>0.941037</td>\n",
       "      <td>0.375016</td>\n",
       "      <td>0.287289</td>\n",
       "      <td>0.339535</td>\n",
       "      <td>0.260639</td>\n",
       "      <td>0.428531</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.572450</td>\n",
       "      <td>0.832650</td>\n",
       "      <td>0.804889</td>\n",
       "      <td>0.798193</td>\n",
       "      <td>0.587664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-28</th>\n",
       "      <td>0.949083</td>\n",
       "      <td>0.558899</td>\n",
       "      <td>0.398814</td>\n",
       "      <td>0.294329</td>\n",
       "      <td>0.252971</td>\n",
       "      <td>0.450259</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.574974</td>\n",
       "      <td>0.833689</td>\n",
       "      <td>0.801182</td>\n",
       "      <td>0.796229</td>\n",
       "      <td>0.545763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-29</th>\n",
       "      <td>0.946741</td>\n",
       "      <td>0.621347</td>\n",
       "      <td>0.509992</td>\n",
       "      <td>0.411795</td>\n",
       "      <td>0.232297</td>\n",
       "      <td>0.462838</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.571669</td>\n",
       "      <td>0.833782</td>\n",
       "      <td>0.799321</td>\n",
       "      <td>0.796593</td>\n",
       "      <td>0.491854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1887 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              AAPL.O     SlowK     SlowD       RSI       ADX       CCI  \\\n",
       "Date                                                                     \n",
       "2010-04-01  0.018837  0.797082  0.851941  0.769280  0.413634  0.658869   \n",
       "2010-06-01  0.016229  0.907128  0.844247  0.536156  0.374250  0.748379   \n",
       "2010-07-01  0.015895  0.111497  0.109400  0.287327  0.221484  0.249224   \n",
       "2010-11-01  0.015491  0.258310  0.233819  0.445504  0.149730  0.329648   \n",
       "2010-12-01  0.013441  0.723159  0.787620  0.558717  0.137354  0.660432   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2018-06-25  0.929088  0.244971  0.261688  0.215016  0.272481  0.283454   \n",
       "2018-06-26  0.942658  0.308398  0.263699  0.201690  0.271435  0.383573   \n",
       "2018-06-27  0.941037  0.375016  0.287289  0.339535  0.260639  0.428531   \n",
       "2018-06-28  0.949083  0.558899  0.398814  0.294329  0.252971  0.450259   \n",
       "2018-06-29  0.946741  0.621347  0.509992  0.411795  0.232297  0.462838   \n",
       "\n",
       "            Aroon Down  Aroon Up       OBV  Chaikin A/D       SMA       EMA  \\\n",
       "Date                                                                          \n",
       "2010-04-01    0.428571  1.000000  0.466485     0.197673  0.021918  0.021476   \n",
       "2010-06-01    0.571429  1.000000  0.417465     0.386577  0.033742  0.034488   \n",
       "2010-07-01    1.000000  0.428571  0.311572     0.333500  0.045891  0.041627   \n",
       "2010-11-01    0.000000  0.285714  0.566643     0.506703  0.070674  0.068125   \n",
       "2010-12-01    0.357143  0.000000  0.561133     0.530421  0.072551  0.072439   \n",
       "...                ...       ...       ...          ...       ...       ...   \n",
       "2018-06-25    1.000000  0.142857  0.572554     0.836229  0.812646  0.803231   \n",
       "2018-06-26    0.928571  0.071429  0.576126     0.836040  0.808467  0.799351   \n",
       "2018-06-27    0.857143  0.000000  0.572450     0.832650  0.804889  0.798193   \n",
       "2018-06-28    0.785714  0.214286  0.574974     0.833689  0.801182  0.796229   \n",
       "2018-06-29    0.714286  0.142857  0.571669     0.833782  0.799321  0.796593   \n",
       "\n",
       "            SMA_EMA_ratio  \n",
       "Date                       \n",
       "2010-04-01       0.436146  \n",
       "2010-06-01       0.269045  \n",
       "2010-07-01       0.950701  \n",
       "2010-11-01       0.680743  \n",
       "2010-12-01       0.398836  \n",
       "...                   ...  \n",
       "2018-06-25       0.652194  \n",
       "2018-06-26       0.645859  \n",
       "2018-06-27       0.587664  \n",
       "2018-06-28       0.545763  \n",
       "2018-06-29       0.491854  \n",
       "\n",
       "[1887 rows x 13 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2010-04-01    1\n",
       "2010-06-01   -1\n",
       "2010-07-01    1\n",
       "2010-11-01   -1\n",
       "2010-12-01    1\n",
       "             ..\n",
       "2018-06-25    1\n",
       "2018-06-26   -1\n",
       "2018-06-27    1\n",
       "2018-06-28   -1\n",
       "2018-06-29   -1\n",
       "Name: direction, Length: 1887, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: Index(['SlowK', 'RSI', 'CCI', 'OBV', 'EMA'], dtype='object')\n",
      "Model accuracy: 0.5238095238095238\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Assuming 'X' contains the features and 'y' contains the target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Selecting the most relevant features using mutual information\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=5)  # Select top 5 features\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "# Train a model using the selected features\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test_selected = selector.transform(X_test)\n",
    "accuracy = model.score(X_test_selected, y_test)\n",
    "\n",
    "# Display the most relevant features and model accuracy\n",
    "print(\"Selected features:\", selected_features)\n",
    "print(\"Model accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 0: acc: 0.476 up: 0.545\n",
      "feature 1: acc: 0.471 up: 0.397\n",
      "feature 2: acc: 0.492 up: 0.37\n",
      "feature 3: acc: 0.437 up: 0.5\n",
      "feature 4: acc: 0.487 up: 0.81\n",
      "feature 5: acc: 0.466 up: 0.423\n",
      "feature 6: acc: 0.5 up: 0.669\n",
      "feature 7: acc: 0.476 up: 0.365\n",
      "feature 8: acc: 0.471 up: 0.011\n",
      "feature 9: acc: 0.471 up: 0.0\n",
      "feature 10: acc: 0.497 up: 0.635\n",
      "feature 11: acc: 0.495 up: 0.643\n",
      "feature 12: acc: 0.492 up: 0.635\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "\n",
    "for i in range(len(apple_data.columns) - 1):\n",
    "    curr_X_train = X_train[:, 1].reshape(-1, 1)\n",
    "    curr_X_test = X_test[:, i].reshape(-1, 1)\n",
    "    model = SVC(kernel='rbf', gamma= 1.0, C= 1.0)\n",
    "    model.fit(curr_X_train, y_train)\n",
    "    pred = model.predict(curr_X_test)\n",
    "    up_percentage = ((pred > 0).sum() / len(pred))\n",
    "    acc = benchmark_accuracy(pred, y_test)\n",
    "    print(f\"feature {i}: acc: {round(acc[0], 3)} up: {round(up_percentage, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 1: acc: 0.529 up: 1.0, features: (0,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.492 up: 0.571, features: (1,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.497 up: 0.587, features: (2,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.511 up: 0.738, features: (3,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.503 up: 0.878, features: (4,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.481 up: 0.571, features: (5,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.489 up: 0.251, features: (6,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.529 up: 1.0, features: (7,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.529 up: 1.0, features: (8,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.529 up: 1.0, features: (9,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.526 up: 0.992, features: (10,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.526 up: 0.992, features: (11,)\n",
      "best acc: 0.5291005291005291 features: (0,)\n",
      "feature 1: acc: 0.532 up: 0.987, features: (12,)\n",
      "best acc: 0.5317460317460317 features: (12,)\n",
      "feature 2: acc: 0.489 up: 0.653, features: (0, 1)\n",
      "best acc: 0.5317460317460317 features: (12,)\n",
      "feature 2: acc: 0.54 up: 0.873, features: (0, 2)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (0, 3)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.481 up: 0.815, features: (0, 4)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.526 up: 0.997, features: (0, 5)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.511 up: 0.537, features: (0, 6)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (0, 7)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.511 up: 0.77, features: (0, 8)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (0, 9)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.532 up: 0.981, features: (0, 10)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.534 up: 0.979, features: (0, 11)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (0, 12)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.489 up: 0.653, features: (1, 0)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.492 up: 0.635, features: (1, 2)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.489 up: 0.574, features: (1, 3)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.5 up: 0.701, features: (1, 4)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.476 up: 0.481, features: (1, 5)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.511 up: 0.685, features: (1, 6)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.526 up: 0.765, features: (1, 7)\n",
      "best acc: 0.5396825396825397 features: (0, 2)\n",
      "feature 2: acc: 0.55 up: 0.815, features: (1, 8)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.508 up: 0.661, features: (1, 9)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.521 up: 0.992, features: (1, 10)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (1, 11)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.452 up: 0.484, features: (1, 12)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.54 up: 0.873, features: (2, 0)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.492 up: 0.635, features: (2, 1)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.505 up: 0.574, features: (2, 3)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.487 up: 0.841, features: (2, 4)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.466 up: 0.524, features: (2, 5)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.503 up: 0.354, features: (2, 6)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.521 up: 0.913, features: (2, 7)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.548 up: 0.828, features: (2, 8)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.545 up: 0.905, features: (2, 9)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (2, 10)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (2, 11)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.46 up: 0.46, features: (2, 12)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (3, 0)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.489 up: 0.574, features: (3, 1)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.505 up: 0.574, features: (3, 2)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.495 up: 0.865, features: (3, 4)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.519 up: 0.82, features: (3, 5)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.484 up: 0.368, features: (3, 6)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.537 up: 0.886, features: (3, 7)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.532 up: 0.981, features: (3, 8)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.537 up: 0.96, features: (3, 9)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (3, 10)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (3, 11)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.484 up: 0.775, features: (3, 12)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.481 up: 0.815, features: (4, 0)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.5 up: 0.701, features: (4, 1)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.487 up: 0.841, features: (4, 2)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.495 up: 0.865, features: (4, 3)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.463 up: 0.606, features: (4, 5)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.5 up: 0.876, features: (4, 6)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.495 up: 0.865, features: (4, 7)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.5 up: 0.876, features: (4, 8)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.511 up: 0.897, features: (4, 9)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (4, 10)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (4, 11)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.495 up: 0.865, features: (4, 12)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.526 up: 0.997, features: (5, 0)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.476 up: 0.481, features: (5, 1)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.466 up: 0.524, features: (5, 2)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.519 up: 0.82, features: (5, 3)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.463 up: 0.606, features: (5, 4)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.481 up: 0.698, features: (5, 6)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.503 up: 0.82, features: (5, 7)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.524 up: 0.963, features: (5, 8)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.513 up: 0.963, features: (5, 9)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.529 up: 0.989, features: (5, 10)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.529 up: 0.989, features: (5, 11)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.481 up: 0.608, features: (5, 12)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.511 up: 0.537, features: (6, 0)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.511 up: 0.685, features: (6, 1)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.503 up: 0.354, features: (6, 2)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.484 up: 0.368, features: (6, 3)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.5 up: 0.876, features: (6, 4)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.481 up: 0.698, features: (6, 5)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.489 up: 0.198, features: (6, 7)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (6, 8)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.495 up: 0.415, features: (6, 9)\n",
      "best acc: 0.5502645502645502 features: (1, 8)\n",
      "feature 2: acc: 0.558 up: 0.881, features: (6, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.556 up: 0.894, features: (6, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.511 up: 0.78, features: (6, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (7, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.526 up: 0.765, features: (7, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.521 up: 0.913, features: (7, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.537 up: 0.886, features: (7, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.495 up: 0.865, features: (7, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.503 up: 0.82, features: (7, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.489 up: 0.198, features: (7, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (7, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (7, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (7, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (7, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.484 up: 0.156, features: (7, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.511 up: 0.77, features: (8, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.55 up: 0.815, features: (8, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.548 up: 0.828, features: (8, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.532 up: 0.981, features: (8, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.5 up: 0.876, features: (8, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.963, features: (8, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (8, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (8, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (8, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (8, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (8, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 0.984, features: (8, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (9, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.508 up: 0.661, features: (9, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.545 up: 0.905, features: (9, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.537 up: 0.96, features: (9, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.511 up: 0.897, features: (9, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.513 up: 0.963, features: (9, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.495 up: 0.415, features: (9, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (9, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (9, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.526 up: 0.997, features: (9, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (9, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 0.984, features: (9, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.532 up: 0.981, features: (10, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.521 up: 0.992, features: (10, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (10, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (10, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (10, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 0.989, features: (10, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.558 up: 0.881, features: (10, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (10, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (10, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.526 up: 0.997, features: (10, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.526 up: 0.992, features: (10, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (10, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.534 up: 0.979, features: (11, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (11, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (11, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (11, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (11, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 0.989, features: (11, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.556 up: 0.894, features: (11, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (11, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (11, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (11, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.526 up: 0.992, features: (11, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (11, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 1.0, features: (12, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.452 up: 0.484, features: (12, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.46 up: 0.46, features: (12, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.484 up: 0.775, features: (12, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.495 up: 0.865, features: (12, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.481 up: 0.608, features: (12, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.511 up: 0.78, features: (12, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.484 up: 0.156, features: (12, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 0.984, features: (12, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.529 up: 0.984, features: (12, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (12, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 2: acc: 0.524 up: 0.995, features: (12, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.751, features: (0, 1, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.632, features: (0, 1, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.839, features: (0, 1, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.778, features: (0, 1, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.5 up: 0.849, features: (0, 1, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.474 up: 0.701, features: (0, 1, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.5 up: 0.521, features: (0, 1, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.574, features: (0, 1, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.542 up: 0.839, features: (0, 1, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.54 up: 0.841, features: (0, 1, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.968, features: (0, 1, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.751, features: (0, 2, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.987, features: (0, 2, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.492 up: 0.831, features: (0, 2, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.519 up: 0.989, features: (0, 2, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.503 up: 0.598, features: (0, 2, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.981, features: (0, 2, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.545 up: 0.974, features: (0, 2, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.984, features: (0, 2, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.995, features: (0, 2, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.992, features: (0, 2, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.966, features: (0, 2, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.632, features: (0, 3, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.987, features: (0, 3, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.508 up: 0.889, features: (0, 3, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 3, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.513 up: 0.651, features: (0, 3, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 3, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.537 up: 0.987, features: (0, 3, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 3, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.997, features: (0, 3, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.997, features: (0, 3, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 3, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.839, features: (0, 4, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.492 up: 0.831, features: (0, 4, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.508 up: 0.889, features: (0, 4, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.471 up: 0.794, features: (0, 4, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.492 up: 0.831, features: (0, 4, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.516 up: 0.923, features: (0, 4, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.46 up: 0.582, features: (0, 4, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.463 up: 0.733, features: (0, 4, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.519 up: 0.989, features: (0, 4, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.521 up: 0.992, features: (0, 4, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.508 up: 0.884, features: (0, 4, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.778, features: (0, 5, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.519 up: 0.989, features: (0, 5, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 5, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.471 up: 0.794, features: (0, 5, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.95, features: (0, 5, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.511 up: 0.971, features: (0, 5, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.468 up: 0.532, features: (0, 5, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.516 up: 0.981, features: (0, 5, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.516 up: 0.944, features: (0, 5, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.516 up: 0.944, features: (0, 5, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.995, features: (0, 5, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.5 up: 0.849, features: (0, 6, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.503 up: 0.598, features: (0, 6, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.513 up: 0.651, features: (0, 6, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.492 up: 0.831, features: (0, 6, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.95, features: (0, 6, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.511 up: 0.537, features: (0, 6, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 6, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 6, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.995, features: (0, 6, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (0, 6, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.519 up: 0.825, features: (0, 6, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.474 up: 0.701, features: (0, 7, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.981, features: (0, 7, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 7, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.516 up: 0.923, features: (0, 7, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.511 up: 0.971, features: (0, 7, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.511 up: 0.537, features: (0, 7, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.778, features: (0, 7, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 7, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (0, 7, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (0, 7, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.958, features: (0, 7, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.5 up: 0.521, features: (0, 8, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.545 up: 0.974, features: (0, 8, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.537 up: 0.987, features: (0, 8, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.46 up: 0.582, features: (0, 8, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.468 up: 0.532, features: (0, 8, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 8, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.778, features: (0, 8, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 8, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.987, features: (0, 8, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.987, features: (0, 8, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.984, features: (0, 8, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.574, features: (0, 9, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.984, features: (0, 9, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 9, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.463 up: 0.733, features: (0, 9, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.516 up: 0.981, features: (0, 9, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 9, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 9, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 9, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (0, 9, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (0, 9, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.987, features: (0, 9, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.542 up: 0.839, features: (0, 10, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.995, features: (0, 10, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.997, features: (0, 10, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.519 up: 0.989, features: (0, 10, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.516 up: 0.944, features: (0, 10, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.995, features: (0, 10, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (0, 10, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.987, features: (0, 10, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (0, 10, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.979, features: (0, 10, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.984, features: (0, 10, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.54 up: 0.841, features: (0, 11, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.992, features: (0, 11, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.997, features: (0, 11, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.521 up: 0.992, features: (0, 11, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.516 up: 0.944, features: (0, 11, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (0, 11, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (0, 11, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.987, features: (0, 11, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (0, 11, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.979, features: (0, 11, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.984, features: (0, 11, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.968, features: (0, 12, 1)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.966, features: (0, 12, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (0, 12, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.508 up: 0.884, features: (0, 12, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.995, features: (0, 12, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.519 up: 0.825, features: (0, 12, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.958, features: (0, 12, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.984, features: (0, 12, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.987, features: (0, 12, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.984, features: (0, 12, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.984, features: (0, 12, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.751, features: (1, 0, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.632, features: (1, 0, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.839, features: (1, 0, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.778, features: (1, 0, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.5 up: 0.849, features: (1, 0, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.474 up: 0.701, features: (1, 0, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.5 up: 0.521, features: (1, 0, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.574, features: (1, 0, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.542 up: 0.839, features: (1, 0, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.54 up: 0.841, features: (1, 0, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.968, features: (1, 0, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.751, features: (1, 2, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.487 up: 0.608, features: (1, 2, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.468 up: 0.706, features: (1, 2, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.476 up: 0.646, features: (1, 2, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.503 up: 0.677, features: (1, 2, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.474 up: 0.632, features: (1, 2, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.545 up: 0.788, features: (1, 2, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.537 up: 0.796, features: (1, 2, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.886, features: (1, 2, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.907, features: (1, 2, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.471 up: 0.603, features: (1, 2, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.632, features: (1, 3, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.487 up: 0.608, features: (1, 3, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.505 up: 0.738, features: (1, 3, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.653, features: (1, 3, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.476 up: 0.413, features: (1, 3, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.854, features: (1, 3, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.54 up: 0.783, features: (1, 3, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.505 up: 0.717, features: (1, 3, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.979, features: (1, 3, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.987, features: (1, 3, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.455 up: 0.497, features: (1, 3, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.839, features: (1, 4, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.468 up: 0.706, features: (1, 4, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.505 up: 0.738, features: (1, 4, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.458 up: 0.563, features: (1, 4, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.495 up: 0.796, features: (1, 4, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.696, features: (1, 4, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.511 up: 0.865, features: (1, 4, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.484 up: 0.839, features: (1, 4, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.937, features: (1, 4, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.937, features: (1, 4, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.458 up: 0.548, features: (1, 4, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.778, features: (1, 5, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.476 up: 0.646, features: (1, 5, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.653, features: (1, 5, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.458 up: 0.563, features: (1, 5, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.484 up: 0.69, features: (1, 5, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.511 up: 0.786, features: (1, 5, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.508 up: 0.651, features: (1, 5, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.513 up: 0.688, features: (1, 5, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.995, features: (1, 5, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.997, features: (1, 5, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.471 up: 0.497, features: (1, 5, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.5 up: 0.849, features: (1, 6, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.503 up: 0.677, features: (1, 6, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.476 up: 0.413, features: (1, 6, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.495 up: 0.796, features: (1, 6, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.484 up: 0.69, features: (1, 6, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.5 up: 0.664, features: (1, 6, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.545 up: 0.937, features: (1, 6, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.513 up: 0.794, features: (1, 6, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.899, features: (1, 6, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.905, features: (1, 6, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.492 up: 0.646, features: (1, 6, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.474 up: 0.701, features: (1, 7, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.474 up: 0.632, features: (1, 7, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.854, features: (1, 7, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.696, features: (1, 7, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.511 up: 0.786, features: (1, 7, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.5 up: 0.664, features: (1, 7, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.513 up: 0.725, features: (1, 7, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.481 up: 0.688, features: (1, 7, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.958, features: (1, 7, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.958, features: (1, 7, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.471 up: 0.566, features: (1, 7, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.5 up: 0.521, features: (1, 8, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.545 up: 0.788, features: (1, 8, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.54 up: 0.783, features: (1, 8, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.511 up: 0.865, features: (1, 8, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.508 up: 0.651, features: (1, 8, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.545 up: 0.937, features: (1, 8, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.513 up: 0.725, features: (1, 8, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.511 up: 0.728, features: (1, 8, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (1, 8, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (1, 8, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.889, features: (1, 8, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.489 up: 0.574, features: (1, 9, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.537 up: 0.796, features: (1, 9, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.505 up: 0.717, features: (1, 9, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.484 up: 0.839, features: (1, 9, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.513 up: 0.688, features: (1, 9, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.513 up: 0.794, features: (1, 9, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.481 up: 0.688, features: (1, 9, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.511 up: 0.728, features: (1, 9, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (1, 9, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (1, 9, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.942, features: (1, 9, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.542 up: 0.839, features: (1, 10, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.886, features: (1, 10, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.979, features: (1, 10, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.937, features: (1, 10, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.995, features: (1, 10, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 0.899, features: (1, 10, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.958, features: (1, 10, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (1, 10, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (1, 10, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.997, features: (1, 10, 11)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (1, 10, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.54 up: 0.841, features: (1, 11, 0)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.532 up: 0.907, features: (1, 11, 2)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.987, features: (1, 11, 3)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.524 up: 0.937, features: (1, 11, 4)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.997, features: (1, 11, 5)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.905, features: (1, 11, 6)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.534 up: 0.958, features: (1, 11, 7)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (1, 11, 8)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.529 up: 1.0, features: (1, 11, 9)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.997, features: (1, 11, 10)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n",
      "feature 3: acc: 0.526 up: 0.992, features: (1, 11, 12)\n",
      "best acc: 0.5582010582010583 features: (6, 10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m curr_X_test \u001b[38;5;241m=\u001b[39m X_test[:, selected_features]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(selected_features))\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m SVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly\u001b[39m\u001b[38;5;124m'\u001b[39m, degree \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m, C\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_X_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(curr_X_test)\n\u001b[1;32m     17\u001b[0m up_percentage \u001b[38;5;241m=\u001b[39m ((pred \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(pred))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[1;32m    319\u001b[0m (\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[0;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "all_features = list(range(len(apple_data.columns) - 1))\n",
    "selected_features = list(itertools.permutations(all_features, 2))\n",
    "\n",
    "best_acc = 0\n",
    "best_selected_features = []\n",
    "\n",
    "for i in range(1, len(apple_data.columns) - 1):\n",
    "    possible_selected_features = list(itertools.permutations(all_features, i))\n",
    "    for selected_features in possible_selected_features:\n",
    "        curr_X_train = X_train[:, selected_features].reshape(-1, len(selected_features))\n",
    "        curr_X_test = X_test[:, selected_features].reshape(-1, len(selected_features))\n",
    "        model = SVC(kernel='poly', degree = 2, C= 0.3)\n",
    "        model.fit(curr_X_train, y_train)\n",
    "        pred = model.predict(curr_X_test)\n",
    "        up_percentage = ((pred > 0).sum() / len(pred))\n",
    "        acc = benchmark_accuracy(pred, y_test)\n",
    "        print(f\"feature {i}: acc: {round(acc[0], 3)} up: {round(up_percentage, 3)}, features: {selected_features}\")\n",
    "\n",
    "        if acc[0] > best_acc:\n",
    "            best_acc = acc[0]\n",
    "            best_selected_features = selected_features\n",
    "        print(f\"best acc: {best_acc} features: {best_selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01883667, 0.79708231],\n",
       "       [0.01622904, 0.9071284 ],\n",
       "       [0.01589451, 0.11149665],\n",
       "       ...,\n",
       "       [0.53766059, 0.68329218],\n",
       "       [0.54156345, 0.72807436],\n",
       "       [0.5410831 , 0.67998392]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
